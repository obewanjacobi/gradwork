---
title: "Homework 5 Solutions"
author: "Jacob S Townson"
date: "March 26, 2018"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
require(knitr)
require(MASS)
require(dplyr)
```

#1)

Before we begin, we must read in our data, and create our variables with the new dataframe. Using these, we can answer the following parts of the question. 

```{r}
HeightData=read.table(...)
g=as.numeric(HeightData$gender=="female")
m=HeightData$mother
f=HeightData$father
y=HeightData$childHeight
X=cbind(g,g*m,g*f,1-g,(1-g)*m,(1-g)*f);dimnames(X)=list(NULL,c('x1','x2','x3','x4','x5','x6'))
X=data.frame(X)
```

The first 5 entries of the dataframe X is given below. These values will be used to do the following problems.

```{r include=FALSE}
kable(X[1:5,])
```


##a)

The MLE of $\beta$ can be found using the following bit of code:

```{r}
X=mutate(X, x7=x2+x5)
X=mutate(X, x8=x3+x6)
height.mod = lm(HeightData$childHeight~X$x1+X$x4+X$x7+X$x8)
```

Note, the reason we add $x_2+x_5=x_7$ and $x_3+x_6=x_8$ is so that we can make sure that the corresponding $\beta$ for $x_2$ and $x_5$ are the same, and the corresponding $\beta$ for $x_3$ and $x_6$ is the same. Now, from the above code, we find that the $\beta$'s are the following:

```{r}
coefs = summary(height.mod)$coefficients
kable(coefs)
```

Thus $\beta = (\beta_1,\beta_2,\beta_3,\beta_4,\beta_5,\beta_6)^{\mathrm{T}}$ where $\beta_1 = $, $\beta_2=\beta_5= $, $\beta_3=\beta_6= $, and $\beta_4 = $. 








#2)










#3)

First we must set up the data. It could have also been read in through a file, however this makes it easier to work from 2 different pc's on the same project.

```{r}
set.seed(123)
x0=1:100/100
x=c(x0,10)
y0=sqrt(1+.1*x0+rnorm(100,sd=.0001))
y=c(y0,0)
hw5_3 = cbind(x,y)
outrm_hw5_3 = hw5_3[1:100,]
```

Note the last two lines of code contain the creation of two dataframes. The first of which contains the entire data set, the second of which removes the outlier. 

##a)

Below is a scatterplot of the data, with the line representing our model going through it. As you can see, the outlier at $x=10$ makes the model seemingly strange for the rest of the data. We will address this in later parts of the problem.

```{r fig.height=4}
model3 = lm(y~x)
plot(x,y, main = "scatterplot for #3a")
abline(model3$coef)
```


##b)

Two of the plots will be supplied below. To calculate the residuals, we use the following code:

```{r}
epsilon.hat=residuals(model3)
r=rstandard(model3)
t=rstudent(model3)
resids=cbind(epsilon.hat,r,t)
kable(resids[101,])
```

This is a table containing the values for the residuals of the outlier in the data. As we can see, these values lead us to believe that the outlier should most likely not be contained, as it throws off the entire model.

Below is the residual plot.

```{r fig.height=4}
plot(model3, which = 1, labels.id = '')
```

Below is the studentized residual plot with the outlier.

```{r fig.height=4}
plot(studres(model3))
```

Below is the studentized residual plot without the outlier.

```{r fig.height=4}
x2=outrm_hw5_3[,1]
y2=outrm_hw5_3[,2]
model3_nout = lm(y2~x2)
plot(studres(model3_nout))
```

##c)

Below is the scatterplot for the data, excluding the outlier. As we can see, when the outlier is removed, the model follows the data extremely well. 

```{r fig.height=4}
plot(x2,y2, main = "scatterplot for #3c")
abline(model3_nout$coef)
```

##d)

Below is the plot for the residuals without the outlier. As we can see, the model follows the line quite well, leading us to believe that the outlier was causing all of the problems in the previous model (suprise). 

```{r fig.heigh=4}
plot(model3_nout, which=1, labels.id='')
```

##e)

```{r fig.height=4}
bc=boxcox(y2~x2,lambda=seq(-2,2,by=.001))
bc$x[which.max(bc$y)]
```

The above code tells us that the value of $\lambda$ that maximizes the log-likelihood function is $\lambda = 1.947$.









